{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b3532ea",
   "metadata": {},
   "source": [
    "**Chapter 5 --- Support Vector Machines (Revision Notebook)**\n",
    "===========================================================\n",
    "\n",
    "**Goal:** compact theory + runnable snippets. Use this as a quick revision: all important concepts are explained in short markdown blocks and demonstrated using small code examples.\n",
    "\n",
    "Files: this notebook contains markdown theory sections and code cells. Run cells in order to reproduce models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955e1ee8",
   "metadata": {},
   "source": [
    "* * * * *\n",
    "\n",
    "1 --- Linear SVM Classification\n",
    "=============================\n",
    "\n",
    "**Idea:**\\\n",
    "Find the decision boundary that **maximizes the margin** --- the distance between classes.\n",
    "\n",
    "**Decision function:**\n",
    "\n",
    "`wᵀx + b = 0`\n",
    "\n",
    "**Margin:**\n",
    "\n",
    "``` margin = 2 / ||w|| ```\n",
    "\n",
    "Support vectors = training points closest to the margin.\\\n",
    "Only these points affect the decision boundary.\n",
    "\n",
    "* * * * *\n",
    "\n",
    "### Linear SVM Example (Iris: 2 features)\n",
    "\n",
    "* * * * *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d4aa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, (2, 3)]     # petal length, petal width\n",
    "y = (iris.target == 2).astype(int)\n",
    "\n",
    "model = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm', LinearSVC(C=1, loss='hinge'))\n",
    "])\n",
    "\n",
    "model.fit(X, y)\n",
    "model.predict([[5.5, 1.7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c126116",
   "metadata": {},
   "source": [
    "2 --- Hard Margin vs Soft Margin SVM\n",
    "==================================\n",
    "\n",
    "### Hard Margin\n",
    "\n",
    "-   No violations allowed\n",
    "\n",
    "-   Only works if data is **perfectly separable**\n",
    "\n",
    "-   Very sensitive to noise and outliers\n",
    "\n",
    "### Soft Margin\n",
    "\n",
    "Use hyperparameter **C** to allow violations.\n",
    "\n",
    "-   **Low C** → wider margin → better generalization\n",
    "\n",
    "-   **High C** → strict margin → fits noise → overfitting\n",
    "\n",
    "* * * * *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772ee9cc",
   "metadata": {},
   "source": [
    "3 --- Feature Scaling is Critical\n",
    "===============================\n",
    "\n",
    "SVMs are **extremely sensitive to feature scale**.\n",
    "\n",
    "Always use:\n",
    "\n",
    "`StandardScaler`\n",
    "\n",
    "before applying SVM.\n",
    "\n",
    "* * * * *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ea85a8",
   "metadata": {},
   "source": [
    "4 --- Nonlinear SVM & Kernels\n",
    "===========================\n",
    "\n",
    "SVM can learn nonlinear boundaries using the **kernel trick**.\n",
    "\n",
    "Common kernels:\n",
    "\n",
    "-   Polynomial kernel\n",
    "\n",
    "-   RBF (Gaussian) kernel\n",
    "\n",
    "-   Sigmoid kernel\n",
    "\n",
    "Kernel trick maps inputs to a higher-dimensional space **implicitly** (no manual feature engineering).\n",
    "\n",
    "* * * * *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e9f0cf",
   "metadata": {},
   "source": [
    "4.1 Polynomial Kernel Example\n",
    "============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c84888a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "X, y = make_moons(n_samples=200, noise=0.2, random_state=0)\n",
    "\n",
    "poly_svm = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm', SVC(kernel='poly', degree=3, C=5))\n",
    "])\n",
    "\n",
    "poly_svm.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32229708",
   "metadata": {},
   "source": [
    "* * * * *\n",
    "\n",
    "4.2 RBF Kernel (Gaussian)\n",
    "=========================\n",
    "\n",
    "**Hyperparameters:**\n",
    "\n",
    "-   `gamma` → controls curvature\n",
    "\n",
    "-   `C` → controls softness of margin\n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "-   **High gamma** → tight, wiggly boundary → overfitting\n",
    "\n",
    "-   **Low gamma** → smoother boundary → underfitting\n",
    "\n",
    "### RBF Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce33565",
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_svm = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm', SVC(kernel='rbf', C=2, gamma=0.1))\n",
    "])\n",
    "\n",
    "rbf_svm.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5063db6e",
   "metadata": {},
   "source": [
    "5 --- SVM Regression (SVR)\n",
    "========================\n",
    "\n",
    "SVM can perform regression by fitting an **epsilon-insensitive tube**.\n",
    "\n",
    "ε = width of the tube.\\\n",
    "Only points outside the tube contribute to the loss.\n",
    "\n",
    "### SVR Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84aa1486",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "import numpy as np\n",
    "\n",
    "X = np.linspace(-3, 3, 50).reshape(-1, 1)\n",
    "y = 0.5 * X[:,0]**2 + np.random.randn(50)\n",
    "\n",
    "svr = SVR(kernel='rbf', C=50, gamma=0.5, epsilon=0.1)\n",
    "svr.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc4cdb4",
   "metadata": {},
   "source": [
    "6 --- Hinge Loss (Under the Hood)\n",
    "===============================\n",
    "\n",
    "Used in SVM classification:\n",
    "\n",
    "`Loss = max(0, 1 - y * (wᵀx))`\n",
    "\n",
    "-   Only penalizes points inside the margin\n",
    "\n",
    "-   Points outside the margin contribute **zero** loss\n",
    "\n",
    "-   Support vectors are exactly the points with positive loss\n",
    "\n",
    "* * * * *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2db8e2",
   "metadata": {},
   "source": [
    "7 --- Practical Notes\n",
    "===================\n",
    "\n",
    "-   **Always scale features** (StandardScaler).\n",
    "\n",
    "-   `LinearSVC` scales well for **large datasets**.\n",
    "\n",
    "-   Kernel SVC is powerful but **slow** for large sample sizes.\n",
    "\n",
    "-   Works extremely well in **high-dimensional** feature spaces.\n",
    "\n",
    "-   SVMs are robust to outliers *only with soft margins* (low C).\n",
    "\n",
    "* * * * *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cfd2a0",
   "metadata": {},
   "source": [
    "8 --- Summary Table (One-Glance)\n",
    "==============================\n",
    "\n",
    "| Concept | Meaning |\n",
    "| --- | --- |\n",
    "| Linear SVM | Fast, works for high-dimensional data |\n",
    "| Polynomial SVM | Good for polynomial-shaped boundaries |\n",
    "| RBF SVM | General non-linear boundary, most popular |\n",
    "| SVR | Regression with epsilon-insensitive margin |\n",
    "| C | Lower C → more regularization |\n",
    "| gamma | Higher gamma → more complex curves |\n",
    "\n",
    "* * * * *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16426629",
   "metadata": {},
   "source": [
    "9 --- Quick Reference\n",
    "===================\n",
    "\n",
    "-   **Linear SVM** = max-margin linear classifier\n",
    "\n",
    "-   **Soft Margin SVM** = adds flexibility with parameter C\n",
    "\n",
    "-   **Kernel SVM** = nonlinear boundaries\n",
    "\n",
    "-   **RBF kernel** = default best choice\n",
    "\n",
    "-   **SVR** = regression version of SVM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
