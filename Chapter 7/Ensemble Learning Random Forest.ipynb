{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "feef10f2",
   "metadata": {},
   "source": [
    "**Chapter 7 --- Ensemble Learning Random Forest (Revision Summary)**\n",
    "=================================================\n",
    "\n",
    "*Simple theory + formulas + minimal runnable snippets*\n",
    "\n",
    "* * * * *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce1df66",
   "metadata": {},
   "source": [
    "**1 --- What Is Ensemble Learning?**\n",
    "==================================\n",
    "\n",
    "Ensemble learning = **combining predictions from multiple models** to get a better overall prediction.\n",
    "\n",
    "Key idea:\n",
    "\n",
    "`Many weak models  →  together become a strong model.`\n",
    "\n",
    "Types of ensembles:\n",
    "\n",
    "-   Averaging methods (Bagging, Random Forests)\n",
    "\n",
    "-   Boosting (AdaBoost, Gradient Boosting)\n",
    "\n",
    "-   Stacking (meta-model on top of base models)\n",
    "\n",
    "Why ensembles work:\n",
    "\n",
    "-   Reduce variance\n",
    "\n",
    "-   Reduce bias (boosting)\n",
    "\n",
    "-   Reduce overfitting (bagging)\n",
    "\n",
    "-   Increase robustness\n",
    "\n",
    "* * * * *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11c2b30",
   "metadata": {},
   "source": [
    "**2 --- Voting Classifiers**\n",
    "==========================\n",
    "\n",
    "Combine predictions of multiple classifiers.\n",
    "\n",
    "Types:\n",
    "\n",
    "**2.1 Hard Voting**\n",
    "-------------------\n",
    "\n",
    "Choose class with **most votes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "906f2e7c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '₁' (U+2081) (271338967.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mŷ = mode(ŷ₁, ŷ₂, ..., ŷ_k)\u001b[39m\n              ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid character '₁' (U+2081)\n"
     ]
    }
   ],
   "source": [
    "ŷ = mode(ŷ₁, ŷ₂, ..., ŷ_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcd1e87",
   "metadata": {},
   "source": [
    "**2.2 Soft Voting**\n",
    "-------------------\n",
    "\n",
    "Average predicted probabilities and choose class with highest mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9edfe8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ŷ = argmax_j (1/n Σ p_j^(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214ea9fb",
   "metadata": {},
   "source": [
    "Soft voting often performs better.\n",
    "\n",
    "### Code Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b020fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "voting = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', LogisticRegression()),\n",
    "        ('svc', SVC(probability=True)),\n",
    "        ('dt', DecisionTreeClassifier())\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "voting.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230b14f9",
   "metadata": {},
   "source": [
    "**3 --- Bagging & Pasting**\n",
    "=========================\n",
    "\n",
    "Train multiple models on **different subsets** of the training set.\n",
    "\n",
    "### Bagging (Bootstrap Aggregating):\n",
    "\n",
    "-   Sample **with replacement**\n",
    "\n",
    "-   Each model sees a different bootstrap sample\n",
    "\n",
    "-   Reduces variance\n",
    "\n",
    "### Pasting:\n",
    "\n",
    "-   Sample **without** replacement\n",
    "\n",
    "Final prediction:\n",
    "\n",
    "-   Classification → majority vote\n",
    "\n",
    "-   Regression → average\n",
    "\n",
    "### Code Example (Bagging):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa2dc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag = BaggingClassifier(\n",
    "    DecisionTreeClassifier(),\n",
    "    n_estimators=100,\n",
    "    max_samples=1.0,\n",
    "    bootstrap=True\n",
    ")\n",
    "\n",
    "bag.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa25798e",
   "metadata": {},
   "source": [
    "**4 --- Out-of-Bag (OOB) Evaluation**\n",
    "===================================\n",
    "\n",
    "Since bagging uses bootstrap samples, ~37% of samples are **not** used in each model's training → these are out-of-bag samples.\n",
    "\n",
    "OOB score = evaluate model on the samples it never saw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e951bcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag = BaggingClassifier(\n",
    "    DecisionTreeClassifier(),\n",
    "    n_estimators=200,\n",
    "    bootstrap=True,\n",
    "    oob_score=True\n",
    ")\n",
    "bag.fit(X, y)\n",
    "print(bag.oob_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c49b6fe",
   "metadata": {},
   "source": [
    "**5 --- Random Forests**\n",
    "======================\n",
    "\n",
    "A Random Forest = bagging + decision trees + randomness in feature selection.\n",
    "\n",
    "Each split chooses:\n",
    "\n",
    "`random subset of features → choose best split among them`\n",
    "\n",
    "Why this helps:\n",
    "\n",
    "-   De-correlates trees\n",
    "\n",
    "-   Reduces variance further\n",
    "\n",
    "-   More stable than plain bagging\n",
    "\n",
    "* * * * *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14379551",
   "metadata": {},
   "source": [
    "**6 --- Random Forest Hyperparameters**\n",
    "=====================================\n",
    "\n",
    "| Parameter | Meaning |\n",
    "| --- | --- |\n",
    "| n_estimators | number of trees |\n",
    "| max_features | number of features considered per split |\n",
    "| max_depth | tree depth limit |\n",
    "| min_samples_split | prevent overfitting |\n",
    "| bootstrap | True = bagging |\n",
    "| oob_score | evaluate without validation split |\n",
    "\n",
    "* * * * *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4c33f1",
   "metadata": {},
   "source": [
    "**7 --- Extra-Trees (Extremely Randomized Trees)**\n",
    "================================================\n",
    "\n",
    "Like Random Forests, but splits are chosen **randomly**, not by best impurity reduction.\n",
    "\n",
    "Benefits:\n",
    "\n",
    "-   Even lower variance\n",
    "\n",
    "-   Faster training\n",
    "\n",
    "Drawback:\n",
    "\n",
    "-   Slight increase in bias\n",
    "\n",
    "Example:\n",
    "\n",
    "`from sklearn.ensemble import ExtraTreesClassifier\n",
    "model = ExtraTreesClassifier(n_estimators=200)\n",
    "model.fit(X_train, y_train)`\n",
    "\n",
    "* * * * *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bad8f9",
   "metadata": {},
   "source": [
    "**8 --- Feature Importance**\n",
    "==========================\n",
    "\n",
    "Random Forests provide feature importance based on impurity reduction.\n",
    "\n",
    "`model.feature_importances_`\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "-   Higher value → feature contributed more to splits\n",
    "\n",
    "* * * * *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a177c69",
   "metadata": {},
   "source": [
    "**9 --- Boosting**\n",
    "================\n",
    "\n",
    "Boosting = sequentially train weak learners, each correcting the errors of the previous ones.\n",
    "\n",
    "Types:\n",
    "\n",
    "-   AdaBoost\n",
    "\n",
    "-   Gradient Boosting\n",
    "\n",
    "-   XGBoost (extension)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b723d89a",
   "metadata": {},
   "source": [
    "**10 --- AdaBoost**\n",
    "=================\n",
    "\n",
    "Each model focuses on mistakes of previous model by increasing sample weights.\n",
    "\n",
    "Weight update rule:\n",
    "\n",
    "`wᵢ := wᵢ * exp(α * I(yᵢ ≠ ŷᵢ))`\n",
    "\n",
    "α = model weight in ensemble.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e0ef8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "model = AdaBoostClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec789c1e",
   "metadata": {},
   "source": [
    "**11 --- Gradient Boosting**\n",
    "==========================\n",
    "\n",
    "Train trees sequentially to fit the **residual errors**:\n",
    "\n",
    "\n",
    "\n",
    "Each new tree tries to predict the residual.\n",
    "\n",
    "`from sklearn.ensemble import GradientBoostingRegressor\n",
    "model = GradientBoostingRegressor()`\n",
    "\n",
    "* * * * *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7419c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "residual = y - predictions_so_far"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7140f3",
   "metadata": {},
   "source": [
    "**12 --- Stochastic Gradient Boosting**\n",
    "=====================================\n",
    "\n",
    "Randomizes:\n",
    "\n",
    "-   row sampling (subsample < 1)\n",
    "\n",
    "-   column sampling\n",
    "\n",
    "Reduces overfitting.\n",
    "\n",
    "* * * * *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd20b7eb",
   "metadata": {},
   "source": [
    "**13 --- XGBoost, LightGBM, CatBoost**\n",
    "====================================\n",
    "\n",
    "Advanced gradient boosting libraries:\n",
    "\n",
    "-   exceptionally fast\n",
    "\n",
    "-   great accuracy\n",
    "\n",
    "-   handle large datasets\n",
    "\n",
    "XGBoost implements:\n",
    "\n",
    "-   regularization\n",
    "\n",
    "-   shrinkage\n",
    "\n",
    "-   weighted quantile sketch\n",
    "\n",
    "* * * * *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583db637",
   "metadata": {},
   "source": [
    "**14 --- Stacking (Stacked Generalization)**\n",
    "==========================================\n",
    "\n",
    "Train base learners → feed predictions into a **meta-model** (e.g., logistic regression).\n",
    "\n",
    "Example:\n",
    "\n",
    "`level 0: Random Forest, SVM, Logistic Regression\n",
    "level 1: Logistic Regression (meta learner)`\n",
    "\n",
    "* * * * *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58037efb",
   "metadata": {},
   "source": [
    "**15 --- Summary Table (One-Glance)**\n",
    "===================================\n",
    "\n",
    "| Method | Core Idea | Strength | Weakness |\n",
    "| --- | --- | --- | --- |\n",
    "| Voting | combine models | simple | needs diverse models |\n",
    "| Bagging | bootstrap + many models | reduces variance | can still overfit |\n",
    "| Random Forest | bagging + random features | strong generalization | slower |\n",
    "| Extra Trees | random splits | very fast | slightly higher bias |\n",
    "| AdaBoost | reweight errors | low bias | sensitive to noise |\n",
    "| Gradient Boosting | fit residuals | high accuracy | overfitting if deep |\n",
    "| XGBoost | optimized GBM | SOTA | more complex |\n",
    "| Stacking | meta-model | flexible | slow to train |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
