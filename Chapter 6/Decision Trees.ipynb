{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7795e765",
   "metadata": {},
   "source": [
    "**Chapter 6 --- Decision Trees (Revision Summary)**\n",
    "=================================================\n",
    "\n",
    "*Simple theory + formulas + minimal runnable snippets*\n",
    "\n",
    "* * * * *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccfe234",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**1 --- What Is a Decision Tree?**\n",
    "================================\n",
    "\n",
    "A decision tree predicts by repeatedly **splitting the data** based on feature thresholds.\n",
    "\n",
    "-   For classification → predicts a **class**\n",
    "\n",
    "-   For regression → predicts a **numeric value**\n",
    "\n",
    "A tree learns **rules** like:\n",
    "\n",
    "`if feature A ≤ 2.7:\n",
    "    if feature B > 1.3:\n",
    "        class = 1\n",
    "    else:\n",
    "        class = 0\n",
    "else:\n",
    "    class = 2`\n",
    "\n",
    "Trees split nodes to make child nodes **purer**.\n",
    "\n",
    "No need for feature scaling.\n",
    "\n",
    "* * * * *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2131e450",
   "metadata": {},
   "source": [
    "**2 --- Decision Tree Classification**\n",
    "====================================\n",
    "\n",
    "**2.1 Impurity Measures**\n",
    "-------------------------\n",
    "\n",
    "A tree chooses splits that reduce impurity.\n",
    "\n",
    "### **Gini Impurity**\n",
    "\n",
    "Most common in sklearn:\n",
    "\n",
    "`Gini = 1 - Σ (pᵢ²)`\n",
    "\n",
    "Minimum = 0 (pure node).\n",
    "\n",
    "### **Entropy (Information Gain)**\n",
    "\n",
    "`Entropy = - Σ (pᵢ log₂ pᵢ)`\n",
    "\n",
    "Information Gain = reduction in entropy.\n",
    "\n",
    "Both produce similar trees.\n",
    "\n",
    "* * * * *\n",
    "**2.2 Classification Code Example**\n",
    "-----------------------------------\n",
    "\n",
    "`from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = DecisionTreeClassifier(\n",
    "    criterion=\"gini\",     # or \"entropy\"\n",
    "    max_depth=4,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)`\n",
    "\n",
    "* * * * *\n",
    "\n",
    "**2.3 Important Classification Hyperparameters**\n",
    "------------------------------------------------\n",
    "\n",
    "| Hyperparameter | Meaning |\n",
    "| --- | --- |\n",
    "| **max_depth** | Maximum depth of the tree (main control for overfitting) |\n",
    "| **min_samples_split** | Minimum samples required to split a node |\n",
    "| **min_samples_leaf** | Minimum samples required in a leaf |\n",
    "| **max_leaf_nodes** | Maximum leaves allowed |\n",
    "| **criterion** | \"gini\" or \"entropy\" |\n",
    "| **class_weight** | Handle class imbalance |\n",
    "\n",
    "### **Overfitting control:**\n",
    "\n",
    "Increase `min_samples_leaf`, reduce `max_depth`.\n",
    "\n",
    "* * * * *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98860d1c",
   "metadata": {},
   "source": [
    "**3 --- Decision Tree Regression**\n",
    "================================\n",
    "\n",
    "Same idea, but predicts a continuous value.\n",
    "\n",
    "* * * * *\n",
    "\n",
    "**3.1 Regression Impurity Measures**\n",
    "------------------------------------\n",
    "\n",
    "Instead of Gini/Entropy, regression trees use **variance**.\n",
    "\n",
    "### **MSE impurity (default)**\n",
    "\n",
    "`impurity = mean( (y - mean(y))² )`\n",
    "\n",
    "A tree selects splits that **reduce MSE**.\n",
    "\n",
    "* * * * *\n",
    "\n",
    "**3.2 Regression Code Example**\n",
    "-------------------------------\n",
    "\n",
    "`from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "reg = DecisionTreeRegressor(\n",
    "    max_depth=5,\n",
    "    min_samples_leaf=4,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "reg.fit(X_train, y_train)\n",
    "pred = reg.predict(X_test)`\n",
    "\n",
    "* * * * *\n",
    "\n",
    "**3.3 Regression Hyperparameters**\n",
    "----------------------------------\n",
    "\n",
    "Same as classification, except criterion options:\n",
    "\n",
    "-   `criterion=\"squared_error\"`\n",
    "\n",
    "-   `criterion=\"friedman_mse\"`\n",
    "\n",
    "-   `criterion=\"absolute_error\"`\n",
    "\n",
    "* * * * *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7e49d5",
   "metadata": {},
   "source": [
    "**4 --- Handling Overfitting in Decision Trees**\n",
    "==============================================\n",
    "\n",
    "Trees **overfit VERY easily**.\n",
    "\n",
    "Use:\n",
    "\n",
    "### **Pruning / Pre-pruning**\n",
    "\n",
    "-   `max_depth`\n",
    "\n",
    "-   `min_samples_split`\n",
    "\n",
    "-   `min_samples_leaf`\n",
    "\n",
    "-   `max_leaf_nodes`\n",
    "\n",
    "### **Post-Pruning**\n",
    "\n",
    "(sklearn supports cost complexity pruning)\n",
    "\n",
    "`ccp_alpha > 0     # bigger alpha = more pruning`\n",
    "\n",
    "Example:\n",
    "\n",
    "`tree_pruned = DecisionTreeClassifier(ccp_alpha=0.01)`\n",
    "\n",
    "* * * * *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778add65",
   "metadata": {},
   "source": [
    "**5 --- Advantages & Limitations**\n",
    "================================\n",
    "\n",
    "### **Advantages**\n",
    "\n",
    "-   Easy to interpret\n",
    "\n",
    "-   No scaling needed\n",
    "\n",
    "-   Handles nonlinear relations\n",
    "\n",
    "-   Handles mixed feature types\n",
    "\n",
    "-   Fast inference\n",
    "\n",
    "### **Limitations**\n",
    "\n",
    "-   High variance (unstable)\n",
    "\n",
    "-   Overfits easily\n",
    "\n",
    "-   Not as accurate as ensemble methods\n",
    "\n",
    "Ensemble methods like **Random Forest**, **Extra Trees**, **Gradient Boosting** fix these issues.\n",
    "\n",
    "* * * * *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dce15fa",
   "metadata": {},
   "source": [
    "**6 --- Visualizing Trees**\n",
    "========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89832e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plot_tree(model, filled=True, feature_names=feature_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd54816",
   "metadata": {},
   "source": [
    "**7 --- Feature Importance**\n",
    "==========================\n",
    "\n",
    "Each tree computes importance based on impurity reduction.\n",
    "\n",
    "\n",
    "Higher = more important.\n",
    "\n",
    "* * * * *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bca642f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c33710",
   "metadata": {},
   "source": [
    "**8 --- Summary Table (One-Glance)**\n",
    "==================================\n",
    "\n",
    "| Type | Split Metric | Output | Notes |\n",
    "| --- | --- | --- | --- |\n",
    "| Classification Tree | Gini or Entropy | Class label | Overfits easily |\n",
    "| Regression Tree | MSE | Numeric value | Same hyperparameters as classification |\n",
    "| Pre-pruning | max_depth, min_samples_leaf | Controls overfitting | Most effective |\n",
    "| Post-pruning | ccp_alpha | Removes weak splits | Very useful |\n",
    "| Feature Importance | impurity reduction | ranking | Good interpretability |\n",
    "\n",
    "* * * * *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da92534",
   "metadata": {},
   "source": [
    "**9 --- Full Minimal Example (Classification)**\n",
    "=============================================\n",
    "\n",
    "* * * * *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4eec6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=4)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3220724e",
   "metadata": {},
   "source": [
    "**10 --- Full Minimal Example (Regression)**\n",
    "=========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede85713",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "reg = DecisionTreeRegressor(max_depth=4)\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "pred = reg.predict(X_test)\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_test, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2047f37b",
   "metadata": {},
   "source": [
    "**How Do Decision Trees Split in Regression?**\n",
    "==============================================\n",
    "\n",
    "Unlike classification trees (which use Gini/Entropy),\\\n",
    "**Regression Trees use MSE (mean squared error) or variance** to choose splits.\n",
    "\n",
    "Everything is based on one idea:\n",
    "\n",
    "> **A good split creates child nodes where the target values are close to each other.**\n",
    "\n",
    "* * * * *\n",
    "\n",
    " **Goal of Regression Trees**\n",
    "===============================\n",
    "\n",
    "For each candidate split:\n",
    "\n",
    "`X_j ≤ threshold`\n",
    "\n",
    "we compute how much it reduces the total error.\n",
    "\n",
    "The attribute **and** the **threshold** giving the **largest error reduction** is selected.\n",
    "\n",
    "Simple.\n",
    "\n",
    "* * * * *\n",
    "\n",
    "**Step 1 --- Define Impurity (Error) of a Node**\n",
    "=================================================\n",
    "\n",
    "For regression, impurity = variance (or MSE):\n",
    "\n",
    "`Impurity(node) = (1/N) Σ (yᵢ - mean(y))²`\n",
    "\n",
    "or simply the variance of values in that node.\n",
    "\n",
    "* * * * *\n",
    "\n",
    "**Step 2 --- Try Every Feature + Every Possible Split Point**\n",
    "==============================================================\n",
    "\n",
    "Given feature `X_j` with continuous values:\n",
    "\n",
    "### How do we get candidate thresholds?\n",
    "\n",
    "We take **sorted unique values** of that column and try **midpoints** between them.\n",
    "\n",
    "Example:\n",
    "\n",
    "`Values: [1, 3, 5]\n",
    "Candidate thresholds: 2, 4`\n",
    "\n",
    "So a split is:\n",
    "\n",
    "`X_j ≤ threshold`\n",
    "\n",
    "Tree tries ALL of them.\n",
    "\n",
    "* * * * *\n",
    "\n",
    "**Step 3 --- For Each Split, Compute Left and Right Node Errors**\n",
    "==================================================================\n",
    "\n",
    "Suppose a split creates:\n",
    "\n",
    "-   Left node: N_L samples\n",
    "\n",
    "-   Right node: N_R samples\n",
    "\n",
    "Compute their impurities:\n",
    "\n",
    "`I_L = impurity(left)\n",
    "I_R = impurity(right)`\n",
    "\n",
    "* * * * *\n",
    "\n",
    "**Step 4 --- Compute Weighted Average Impurity After Split**\n",
    "=============================================================\n",
    "\n",
    "This is the **post-split impurity**:\n",
    "\n",
    "`WeightedImpurity = (N_L / N_total) * I_L  +  (N_R / N_total) * I_R`\n",
    "\n",
    "* * * * *\n",
    "\n",
    "**Step 5 --- Compute Impurity Reduction (the \"Gain\")**\n",
    "=======================================================\n",
    "\n",
    "`Gain = Impurity_parent - WeightedImpurity`\n",
    "\n",
    "The tree chooses the split with **maximum gain**.\n",
    "\n",
    "* * * * *\n",
    "\n",
    " FULL FORMULA \n",
    "================\n",
    "\n",
    "Let parent node impurity be:\n",
    "\n",
    "`I_parent = (1/N) Σ (yᵢ - mean(y_parent))²`\n",
    "\n",
    "Then best split = argmax over all (j, threshold):\n",
    "\n",
    "`Gain(j, threshold) = I_parent - (N_L/N)*I_L - (N_R/N)*I_R`\n",
    "\n",
    "That's it!\n",
    "\n",
    "* * * * *\n",
    "\n",
    " **Intuition Summary**\n",
    "========================\n",
    "\n",
    "-   Try all features\n",
    "\n",
    "-   Try all possible thresholds\n",
    "\n",
    "-   Compute how much the split reduces variance\n",
    "\n",
    "-   Choose the split that makes child nodes purer (less spread)\n",
    "\n",
    "Continuous variables are handled naturally because split thresholds are just numeric points.\n",
    "\n",
    "* * * * *\n",
    "\n",
    " Example (Small Numbers)\n",
    "==========================\n",
    "\n",
    "Suppose y-values in a node:\n",
    "\n",
    "`[3, 4, 7, 8]`\n",
    "\n",
    "Try split at <=4:\n",
    "\n",
    "Left: [3,4]\\\n",
    "Right: [7,8]\n",
    "\n",
    "Variance(left) = low\\\n",
    "Variance(right) = low\n",
    "\n",
    "This is a good split.\n",
    "\n",
    "Try split at <=7:\n",
    "\n",
    "Left: [3,4,7]\\\n",
    "Right: [8]\n",
    "\n",
    "Left variance = higher\\\n",
    "Right variance = zero\\\n",
    "But weighted average variance = worse\n",
    "\n",
    "So <=4 is better.\n",
    "\n",
    "* * * * *\n",
    "\n",
    " Sklearn Equivalent\n",
    "=====================\n",
    "\n",
    "Scikit-Learn uses exactly this, with criterion:\n",
    "\n",
    "`criterion=\"squared_error\"`\n",
    "\n",
    "\n",
    "Final Understanding:\n",
    "=======================\n",
    "\n",
    "### **For regression:**\n",
    "\n",
    "-   Impurity = MSE / variance\n",
    "\n",
    "-   Split chosen based on maximum **variance reduction**\n",
    "\n",
    "-   Continuous variables → try all possible numeric thresholds\n",
    "\n",
    "-   Best split = mean reduction in spread of y-values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ba9ac3",
   "metadata": {},
   "source": [
    "\n",
    " Minimal Code: Print Best Split Manually\n",
    "=========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a4978c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y = np.array([3, 4, 7, 8])\n",
    "X = np.array([2, 4, 6, 8])  # feature\n",
    "\n",
    "def variance(x):\n",
    "    return np.mean((x - np.mean(x))**2)\n",
    "\n",
    "best_gain = -1\n",
    "best_thresh = None\n",
    "\n",
    "parent_imp = variance(y)\n",
    "\n",
    "for t in np.unique(X):\n",
    "    left = y[X <= t]\n",
    "    right = y[X > t]\n",
    "    if len(left)==0 or len(right)==0:\n",
    "        continue\n",
    "    wimp = (len(left)*variance(left) + len(right)*variance(right)) / len(y)\n",
    "    gain = parent_imp - wimp\n",
    "    if gain > best_gain:\n",
    "        best_gain = gain\n",
    "        best_thresh = t\n",
    "\n",
    "best_thresh, best_gain"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
